{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introducing automatic optimization - deep learning framework"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ea8f5ac73140c9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-30T19:11:36.432297523Z",
     "start_time": "2023-08-30T19:11:36.418374791Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np # noqa: E402\n",
    "from dl_framework.tensor import Tensor # noqa: E402\n",
    "from dl_framework.optimisers import SGD # noqa: E402\n",
    "from dl_framework.layers import Linear, Sequential, Tanh, Sigmoid, Embedding, RNNCell # noqa: E402\n",
    "from dl_framework.loss import MSE, CrossEntropy # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9191ad114d830877"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True,  True,  True,  True])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "b.grad.data == np.array([-2,-2,-2,-2,-2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T19:11:39.156722802Z",
     "start_time": "2023-08-30T19:11:39.123810952Z"
    }
   },
   "id": "e5bee7a278871ba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fe392e7b27d558a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.63845013]\n",
      "[118.90031279]\n",
      "[3149.2825024]\n",
      "[2.32355657e+08]\n",
      "[1.79813859e+23]\n",
      "[8.37034314e+67]\n",
      "[8.44466488e+201]\n",
      "[inf]\n",
      "[nan]\n",
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m_zsuetam/Programming/machine-learning/grokking-deep-learning/notebooks/dl_framework/tensor.py:111: RuntimeWarning: overflow encountered in multiply\n",
      "  return Tensor(self.data * other.data, autograd=True, creators=[self, other], creation_op=\"mul\")\n",
      "/tmp/ipykernel_44670/1012030499.py:19: RuntimeWarning: invalid value encountered in multiply\n",
      "  w_.grad.data *= 0\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = [\n",
    "    Tensor(np.random.randn(2,3), autograd=True),\n",
    "    Tensor(np.random.randn(3,1), autograd=True)\n",
    "]\n",
    "\n",
    "for i in range(10):\n",
    "    # predict: \n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    # compare:\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    # learn:\n",
    "    \n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "        \n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:57:08.721394099Z",
     "start_time": "2023-08-28T22:57:08.660530708Z"
    }
   },
   "id": "dd0ad4c9c6f80efd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding automacic optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "424c04366f952be5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.01333338]\n",
      "[5.14996252]\n",
      "[0.07354472]\n",
      "[0.00206634]\n",
      "[0.00057971]\n",
      "[0.00021449]\n",
      "[7.91933347e-05]\n",
      "[2.91593458e-05]\n",
      "[1.07185403e-05]\n",
      "[3.93592422e-06]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = [\n",
    "    Tensor(np.random.randn(2,3), autograd=True),\n",
    "    Tensor(np.random.randn(3,1), autograd=True)\n",
    "]\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    # predict: \n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    # compare:\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    # learn:\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "        \n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:57:08.721671874Z",
     "start_time": "2023-08-28T22:57:08.701897029Z"
    }
   },
   "id": "3ce6893a8a214ba7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2e43174b4d667a"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60298668]\n",
      "[0.4046423]\n",
      "[0.25937405]\n",
      "[0.16806312]\n",
      "[0.11504623]\n",
      "[0.08385281]\n",
      "[0.06443996]\n",
      "[0.05159964]\n",
      "[0.04264263]\n",
      "[0.03611468]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2,3),\n",
    "    Tanh(),\n",
    "    Linear(3,1),\n",
    "    Sigmoid(),\n",
    "])\n",
    "\n",
    "loss_func = MSE()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    # predict: \n",
    "    pred = model.forward(data)\n",
    "    # compare:\n",
    "    loss = loss_func.forward(pred, target)\n",
    "    # learn:\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "        \n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:59:29.305608661Z",
     "start_time": "2023-08-28T22:59:29.263216143Z"
    }
   },
   "id": "6e818e024bf5a612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "117fa4f71df527a4"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3476557905994486\n",
      "0.94522014909927\n",
      "0.7137810427562177\n",
      "0.5648645150998857\n",
      "0.45972076269814255\n",
      "0.3814834100264345\n",
      "0.32156384376170194\n",
      "0.2748283464255008\n",
      "0.23785275500108136\n",
      "0.20821862904375293\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(3,3),\n",
    "    Tanh(),\n",
    "    Linear(3,4),\n",
    "])\n",
    "\n",
    "loss_func = CrossEntropy()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    # predict: \n",
    "    pred = model.forward(data)\n",
    "    # compare:\n",
    "    loss = loss_func.forward(pred, target)\n",
    "    # learn:\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "        \n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T23:02:16.950208706Z",
     "start_time": "2023-08-28T23:02:16.941391128Z"
    }
   },
   "id": "6b7d5b5b193c77d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "791bb8262f172289"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "f = open('../data/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "\n",
    "data = np.array(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:57:08.791353532Z",
     "start_time": "2023-08-28T22:57:08.748863068Z"
    }
   },
   "id": "f7b6cad82ef418c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.516188684878311 % Correct: 0.0\n",
      "Loss: 0.18015247092953957 % Correct: 0.21\n",
      "Loss: 0.1615806150053133 % Correct: 0.31\n",
      "Loss: 0.14675377673074025 % Correct: 0.36\n",
      "Loss: 0.14176057097488015 % Correct: 0.36\n"
     ]
    }
   ],
   "source": [
    "embed = Embedding(vocab_size=len(vocab),dim=16)\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "\n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "    target = Tensor(data[0:batch_size,t+1], autograd=True)    \n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    if(iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print(\"Loss:\",total_loss / (len(data)/batch_size),\"% Correct:\",p_correct)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:57:11.471825320Z",
     "start_time": "2023-08-28T22:57:08.748907973Z"
    }
   },
   "id": "edd20f4b4558c6d9"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: - mary moved to the \n",
      "True: bathroom.\n",
      "Pred: hallway.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "target = Tensor(data[0:batch_size,t+1], autograd=True)    \n",
    "loss = criterion.forward(output, target)\n",
    "\n",
    "ctx = \"\"\n",
    "for idx in data[0:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + \" \"\n",
    "print(\"Context:\",ctx)\n",
    "print(\"True:\",vocab[target.data[0]])\n",
    "print(\"Pred:\", vocab[output.data.argmax()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T22:57:39.291660035Z",
     "start_time": "2023-08-28T22:57:39.281353059Z"
    }
   },
   "id": "5ed9f3c41aa29175"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
