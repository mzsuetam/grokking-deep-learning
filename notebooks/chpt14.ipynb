{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80fefcf0cbc78b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd99d224f83c1f57",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.377934373Z",
     "start_time": "2023-09-05T09:37:41.253321669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "import numpy as np  # noqa: E402\n",
    "from dl_framework.tensor import Tensor  # noqa: E402\n",
    "from dl_framework.optimisers import SGD  # noqa: E402\n",
    "from dl_framework.layers import Embedding, RNNCell, LSTMCell  # noqa: E402\n",
    "from dl_framework.loss import CrossEntropy  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab24b157c4d989",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ce6f3a09b66fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.497694478Z",
     "start_time": "2023-09-05T09:37:41.366203524Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/shakespeare/shakespeare.txt', 'r') as f:\n",
    "    raw = f.read()\n",
    "    vocab = list(set(raw))\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "indices = np.array([word2idx[w] for w in raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82bcc923aefa4c",
   "metadata": {},
   "source": [
    "## Model with simple RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ea8cd8fdaba12f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.550700685Z",
     "start_time": "2023-09-05T09:37:41.500274973Z"
    }
   },
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optimiser = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / batch_size))\n",
    "\n",
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47333acf79070c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.551362750Z",
     "start_time": "2023-09-05T09:37:41.545770019Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2idx[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        #         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8518c7afd99b7fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.551498606Z",
     "start_time": "2023-09-05T09:37:41.546013487Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs=100):\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = []\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if t == 0:\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.data\n",
    "            log = '\\r Iter: ' + str(e)\n",
    "            log += ' - Batch ' + str(batch_i + 1) + '/' + str(n_batches)\n",
    "            log += ' - Loss: ' + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            if batch_i == 0:\n",
    "                log += ' - ' + generate_sample(70, '\\n').replace('\\n', ' ')\n",
    "            if batch_i % 10 == 0 or batch_i - 1 == n_batches:\n",
    "                print(log)\n",
    "        optimiser.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d4b957692ea849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.551638930Z",
     "start_time": "2023-09-05T09:37:41.546192353Z"
    }
   },
   "outputs": [],
   "source": [
    "train_simple_rnn = False\n",
    "\n",
    "if train_simple_rnn:\n",
    "    train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9ed10c96fd90af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.551757773Z",
     "start_time": "2023-09-05T09:37:41.546301970Z"
    }
   },
   "outputs": [],
   "source": [
    "if train_simple_rnn:\n",
    "    print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a29a465f5d338",
   "metadata": {},
   "source": [
    "## Model with LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bccba8907193a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.591858001Z",
     "start_time": "2023-09-05T09:37:41.546435290Z"
    }
   },
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_outputs=len(vocab))\n",
    "model.w_ho.weight.data *= 0  # helps with training\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optimiser = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / batch_size))\n",
    "\n",
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361d72ef133e4f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.592243115Z",
     "start_time": "2023-09-05T09:37:41.589597488Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2idx[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd29942291147819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.638280081Z",
     "start_time": "2023-09-05T09:37:41.592698993Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs=100):\n",
    "    min_loss = 1000\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "\n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True),\n",
    "                      Tensor(hidden[1].data, autograd=True))\n",
    "            losses = []\n",
    "            \n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                \n",
    "                if t == 0:\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i + 1))\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                # print(generate_sample(n=70, init_char='\\n').replace('\\n', ''))\n",
    "            log = '\\r Iter: ' + str(e)\n",
    "            log += ' - Batch ' + str(batch_i + 1) + '/' + str(batches_to_train)\n",
    "            log += ' - Min Loss: ' + str(min_loss)\n",
    "            log += ' - Loss: ' + str(epoch_loss)\n",
    "            if batch_i == 0:\n",
    "                log += ' - ' + generate_sample(n=70, init_char='\\n').replace('\\n', ' ')\n",
    "            if batch_i % 10 == 0 or batch_i - 1 == n_batches:\n",
    "                print(log)\n",
    "        optimiser.alpha *= 0.99            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c5e8e6ef58ee91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:41.795484387Z",
     "start_time": "2023-09-05T09:37:41.633608042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "pickle_path = '../models/lstm_model.pkl'\n",
    "\n",
    "if not os.path.exists(pickle_path):\n",
    "    train(epochs=100)\n",
    "\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        network_model = [model, embed, word2idx, idx2word]\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "        \n",
    "    print(\"Model trained and saved successfully\")\n",
    "        \n",
    "else:\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        network_model = pickle.load(f)\n",
    "        model = network_model[0]\n",
    "        embed = network_model[1]\n",
    "        word2idx = network_model[2]\n",
    "        idx2word = network_model[3]\n",
    "        vocab = list(word2idx.keys())\n",
    "    \n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25163a86377735a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T09:37:47.265312962Z",
     "start_time": "2023-09-05T09:37:41.786006878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is me them from the state.\n",
      "\n",
      "But there is my lord, in the threrer: the did for what thou did grave the set precklersed say you the fair work, the sear'd in here, stranger the prerest thou will rere story for the set mistress.\n",
      "\n",
      "First Must the set them from the set with the seem them for sun them from the serving strembert them from the persaked yourselves the did sled speak the straight the day, but of the set them for the pardof the sear'd from the dangery to make a forth the sear's the daughter, streat the set them servers of more for world so dridders the persery for that the presers, strain the present in the precorthin do no more from the set with his dead me in the season me the press the seasound to suds the dreas the set wear, the precory for the preceavers of my form,\n",
      "As or the preconding up--burth the did strainess the dreat of my graighter, sicomportune shall we threat the daughter, silent in my hand I speak the set with the strangere, storn in the world inderer:\n",
      "Nor I did I am path to sleepy lord, there is my forther this searners, and the presentle great of my reding: the preather, which thou did world for the present are set them for that the set them for the perse, my lord, in the threre for we wine on the set of makes them straight with the daught thou did gentle letter the presersion in here, so were not set were not see the danger this sole sid the dreat the present the day your was wear the set them for the persaked users, straight the server them for the precklore what thou did speak the staters them for the persed yet is my ford, stranger them from the serving woman, which is the set them from the servave the perse, I did for this leard, shall the father state:\n",
      "Or in the seath the precerest the persies the precorthou are the prected in the set them for this save thy for the did strain\n",
      "And there and the present and stay wint upon your son, so word, there is my lord, with the did sleepy father this sin the sear'd in this sight.\n",
      "What thou the fa\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
