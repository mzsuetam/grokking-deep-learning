{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80fefcf0cbc78b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd99d224f83c1f57",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "import numpy as np  # noqa: E402\n",
    "from dl_framework.tensor import Tensor  # noqa: E402\n",
    "from dl_framework.optimisers import SGD  # noqa: E402\n",
    "from dl_framework.layers import Embedding, RNNCell, LSTMCell  # noqa: E402\n",
    "from dl_framework.loss import CrossEntropy  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab24b157c4d989",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce6f3a09b66fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/shakespeare/shakespeare.txt', 'r') as f:\n",
    "    raw = f.read()\n",
    "    vocab = list(set(raw))\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "indices = np.array([word2idx[w] for w in raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82bcc923aefa4c",
   "metadata": {},
   "source": [
    "## Model with simple RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea8cd8fdaba12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optimiser = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / batch_size))\n",
    "\n",
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47333acf79070c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2idx[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        #         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518c7afd99b7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100):\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = []\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if t == 0:\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.data\n",
    "            log = '\\r Iter: ' + str(e)\n",
    "            log += ' - Batch ' + str(batch_i + 1) + '/' + str(n_batches)\n",
    "            log += ' - Loss: ' + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            if batch_i == 0:\n",
    "                log += ' - ' + generate_sample(70, '\\n').replace('\\n', ' ')\n",
    "            if batch_i % 10 == 0 or batch_i - 1 == n_batches:\n",
    "                print(log)\n",
    "        optimiser.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4b957692ea849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ed10c96fd90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a29a465f5d338",
   "metadata": {},
   "source": [
    "## Model with LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccba8907193a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_outputs=len(vocab))\n",
    "model.w_ho.weight.data *= 0  # helps with training\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optimiser = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / batch_size))\n",
    "\n",
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d72ef133e4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2idx[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29942291147819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100):\n",
    "    min_loss = 1000\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "\n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True),\n",
    "                      Tensor(hidden[1].data, autograd=True))\n",
    "            losses = []\n",
    "            \n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                \n",
    "                if t == 0:\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i + 1))\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                # print(generate_sample(n=70, init_char='\\n').replace('\\n', ''))\n",
    "            log = '\\r Iter: ' + str(e)\n",
    "            log += ' - Batch ' + str(batch_i + 1) + '/' + str(batches_to_train)\n",
    "            log += ' - Min Loss: ' + str(min_loss)\n",
    "            log += ' - Loss: ' + str(epoch_loss)\n",
    "            if batch_i == 0:\n",
    "                log += ' - ' + generate_sample(n=70, init_char='\\n').replace('\\n', ' ')\n",
    "            if batch_i % 10 == 0 or batch_i - 1 == n_batches:\n",
    "                print(log)\n",
    "        optimiser.alpha *= 0.99            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5e8e6ef58ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25163a86377735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de89afa2dbd2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4d05ff9e7e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_pickle = True\n",
    "if load_from_pickle:\n",
    "    with open('lstm_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
